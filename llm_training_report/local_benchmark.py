#!/usr/bin/env python3
"""
æœ¬åœ°åŸºå‡†æµ‹è¯• - æ— éœ€ä¸‹è½½å¤§æ¨¡å‹
æµ‹è¯•è®¡ç®—æ€§èƒ½ã€å†…å­˜å¸¦å®½ç­‰
"""

import os
import time
import json
import torch
import psutil
import numpy as np
from datetime import datetime

REPORT_DIR = "/Users/hans/.openclaw/workspace/llm_training_report"

def log(msg):
    print(f"â° {datetime.now().strftime('%H:%M:%S')} | {msg}")

def get_mem():
    return psutil.Process().memory_info().rss / (1024**3)

# ============================================================================
# åŸºå‡† 1: å¼ é‡è¿ç®—æ€§èƒ½
# ============================================================================
def benchmark_matmul():
    log("ğŸ“Š åŸºå‡† 1: çŸ©é˜µä¹˜æ³•æ€§èƒ½")
    
    sizes = [512, 1024, 2048]
    results = []
    
    for size in sizes:
        # å‡†å¤‡æ•°æ®
        a = torch.randn(size, size, device='mps')
        b = torch.randn(size, size, device='mps')
        
        # é¢„çƒ­
        for _ in range(3):
            _ = torch.matmul(a, b)
        
        # è®¡æ—¶
        torch.mps.synchronize()
        start = time.perf_counter()
        
        for _ in range(10):
            _ = torch.matmul(a, b)
        
        torch.mps.synchronize()
        end = time.perf_counter()
        
        elapsed = (end - start) / 10
        flops = (size ** 3 * 2) / elapsed / 1e9  # GFLOPS
        
        results.append({
            "size": size,
            "time_ms": elapsed * 1000,
            "gflops": flops
        })
        log(f"  {size}x{size}: {elapsed*1000:.2f}ms, {flops:.1f} GFLOPS")
    
    return {"status": "success", "results": results}

# ============================================================================
# åŸºå‡† 2: å†…å­˜å¸¦å®½
# ============================================================================
def benchmark_memory():
    log("ğŸ“Š åŸºå‡† 2: å†…å­˜å¸¦å®½æµ‹è¯•")
    
    sizes = [1024, 2048, 4096]
    results = []
    
    for size in sizes:
        # åˆ›å»ºå¤§å¼ é‡
        a = torch.randn(size, size, device='mps')
        
        # é¢„çƒ­
        for _ in range(3):
            _ = a * 2
        
        # æµ‹è¯•èµ‹å€¼æ“ä½œ
        torch.mps.synchronize()
        start = time.perf_counter()
        
        for _ in range(100):
            a = a * 2
            a = a / 2
        
        torch.mps.synchronize()
        end = time.perf_counter()
        
        elapsed = (end - start) / 100
        bandwidth_gbps = (size ** 2 * 4 * 2) / elapsed / 1e9  # GB/s (float32 = 4 bytes)
        
        results.append({
            "size": size,
            "time_ms": elapsed * 1000,
            "gbps": bandwidth_gbps
        })
        log(f"  {size}x{size}: {elapsed*1000:.2f}ms, {bandwidth_gbps:.1f} GB/s")
    
    return {"status": "success", "results": results}

# ============================================================================
# åŸºå‡† 3: ç¥ç»ç½‘ç»œ Forward Pass
# ============================================================================
def benchmark_forward():
    log("ğŸ“Š åŸºå‡† 3: ç¥ç»ç½‘ç»œ Forward Pass")
    
    configs = [
        {"layers": 3, "hidden": 256, "batch": 32},
        {"layers": 6, "hidden": 512, "batch": 16},
        {"layers": 12, "hidden": 768, "batch": 8},
    ]
    results = []
    
    for cfg in configs:
        layers = []
        in_dim = 128
        for i in range(cfg["layers"]):
            layers.append(torch.nn.Linear(in_dim, cfg["hidden"]))
            layers.append(torch.nn.ReLU())
            in_dim = cfg["hidden"]
        layers.append(torch.nn.Linear(in_dim, 10))
        
        model = torch.nn.Sequential(*layers).to('mps')
        x = torch.randn(cfg["batch"], 128, device='mps')
        
        # é¢„çƒ­
        for _ in range(3):
            _ = model(x)
        
        torch.mps.synchronize()
        start = time.perf_counter()
        
        for _ in range(50):
            _ = model(x)
        
        torch.mps.synchronize()
        end = time.perf_counter()
        
        elapsed = (end - start) / 50 * 1000  # ms
        
        results.append({
            "config": f"{cfg['layers']}L-{cfg['hidden']}H",
            "batch": cfg["batch"],
            "time_ms": elapsed
        })
        log(f"  {cfg['layers']}å±‚/{cfg['hidden']}éšè—: {elapsed:.2f}ms/batch")
    
    return {"status": "success", "results": results}

# ============================================================================
# åŸºå‡† 4: ç¥ç»ç½‘ç»œ Backward Pass
# ============================================================================
def benchmark_backward():
    log("ğŸ“Š åŸºå‡† 4: ç¥ç»ç½‘ç»œè®­ç»ƒ (Forward + Backward)")
    
    class SimpleNet(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = torch.nn.ModuleList([
                torch.nn.Linear(256, 512),
                torch.nn.ReLU(),
                torch.nn.Linear(512, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 128),
                torch.nn.Linear(128, 10)
            ])
        
        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x
    
    model = SimpleNet().to('mps')
    optimizer = torch.optim.Adam(model.parameters())
    criterion = torch.nn.CrossEntropyLoss()
    
    batch_sizes = [8, 16, 32]
    results = []
    
    for batch_size in batch_sizes:
        x = torch.randn(batch_size, 256, device='mps')
        y = torch.randint(0, 10, (batch_size,), device='mps')
        
        # é¢„çƒ­
        for _ in range(3):
            optimizer.zero_grad()
            out = model(x)
            loss = criterion(out, y)
            loss.backward()
        
        torch.mps.synchronize()
        start = time.perf_counter()
        
        for _ in range(20):
            optimizer.zero_grad()
            out = model(x)
            loss = criterion(out, y)
            loss.backward()
            optimizer.step()
        
        torch.mps.synchronize()
        end = time.perf_counter()
        
        elapsed = (end - start) / 20 * 1000  # ms
        
        results.append({
            "batch_size": batch_size,
            "time_ms": elapsed,
            "samples_per_sec": batch_size / (elapsed / 1000)
        })
        log(f"  Batch {batch_size}: {elapsed:.2f}ms, {batch_size/(elapsed/1000):.1f} samples/s")
    
    return {"status": "success", "results": results}

# ============================================================================
# åŸºå‡† 5: æ¿€æ´»å‡½æ•°æ€§èƒ½
# ============================================================================
def benchmark_activations():
    log("ğŸ“Š åŸºå‡† 5: æ¿€æ´»å‡½æ•°æ€§èƒ½")
    
    size = 2048
    x = torch.randn(32, size, size, device='mps')
    
    activations = [
        ("ReLU", torch.nn.ReLU()),
        ("GELU", torch.nn.GELU()),
        ("SiLU", torch.nn.SiLU()),
        ("Softmax", torch.nn.Softmax(dim=-1)),
    ]
    results = []
    
    for name, act in activations:
        # é¢„çƒ­
        for _ in range(3):
            _ = act(x)
        
        torch.mps.synchronize()
        start = time.perf_counter()
        
        for _ in range(50):
            _ = act(x)
        
        torch.mps.synchronize()
        end = time.perf_counter()
        
        elapsed = (end - start) / 50 * 1000
        
        results.append({
            "function": name,
            "time_ms": elapsed
        })
        log(f"  {name}: {elapsed:.2f}ms")
    
    return {"status": "success", "results": results}

# ============================================================================
# åŸºå‡† 6: æ··åˆç²¾åº¦æµ‹è¯•
# ============================================================================
def benchmark_fp16():
    log("ğŸ“Š åŸºå‡† 6: æ··åˆç²¾åº¦ (FP16 vs FP32)")
    
    size = 1024
    results = []
    
    # FP32
    a32 = torch.randn(size, size, device='mps', dtype=torch.float32)
    b32 = torch.randn(size, size, device='mps', dtype=torch.float32)
    
    torch.mps.synchronize()
    start = time.perf_counter()
    for _ in range(20):
        _ = torch.matmul(a32, b32)
    torch.mps.synchronize()
    time32 = (time.perf_counter() - start) / 20 * 1000
    
    # FP16
    a16 = a32.half()
    b16 = b32.half()
    
    torch.mps.synchronize()
    start = time.perf_counter()
    for _ in range(20):
        _ = torch.matmul(a16, b16)
    torch.mps.synchronize()
    time16 = (time.perf_counter() - start) / 20 * 1000
    
    speedup = time32 / time16
    
    results.append({"dtype": "FP32", "time_ms": time32})
    results.append({"dtype": "FP16", "time_ms": time16})
    results.append({"speedup": speedup})
    
    log(f"  FP32: {time32:.2f}ms")
    log(f"  FP16: {time16:.2f}ms")
    log(f"  åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    return {"status": "success", "results": results}

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    print("\n" + "="*70)
    print("    ğŸš€ LLM è®­ç»ƒç¯å¢ƒæœ¬åœ°åŸºå‡†æµ‹è¯•")
    print("    Apple M2 Ultra (64GB) - MPS æ€§èƒ½æµ‹è¯•")
    print("="*70 + "\n")
    
    results = {}
    
    # æ£€æŸ¥ MPS
    log(f"ğŸ”§ MPS çŠ¶æ€: {torch.backends.mps.is_available()}")
    log(f"ğŸ”§ è®¾å¤‡: mps (Apple Silicon)\n")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmarks = [
        ("MatMul", benchmark_matmul),
        ("Memory", benchmark_memory),
        ("Forward", benchmark_forward),
        ("Backward", benchmark_backward),
        ("Activations", benchmark_activations),
        ("FP16", benchmark_fp16),
    ]
    
    for name, func in benchmarks:
        try:
            log(f"\n{'='*50}")
            result = func()
            results[name] = result
        except Exception as e:
            log(f"âŒ {name} å¤±è´¥: {e}")
            results[name] = {"status": "error", "error": str(e)}
    
    # æ±‡æ€»
    print("\n" + "="*70)
    print("    ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*70)
    
    for name, result in results.items():
        status = "âœ…" if result.get("status") == "success" else "âŒ"
        print(f"{status} {name}")
    
    # ä¿å­˜
    output_file = os.path.join(REPORT_DIR, "local_benchmark_results.json")
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nğŸ“ ç»“æœä¿å­˜: {output_file}")
    
    return results

if __name__ == "__main__":
    main()
