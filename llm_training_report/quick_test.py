#!/usr/bin/env python3
"""
ç²¾ç®€ç‰ˆ LLM è®­ç»ƒç¯å¢ƒæµ‹è¯•
ä¸“æ³¨äºæœ¬åœ°å¯ç”¨åŠŸèƒ½ï¼Œå¿«é€ŸéªŒè¯
"""

import os
import sys
import time
import json
import torch
import psutil
from datetime import datetime

REPORT_DIR = "/Users/hans/.openclaw/workspace/llm_training_report"

def log(msg):
    print(f"â° {datetime.now().strftime('%H:%M:%S')} | {msg}")

def get_mem():
    return psutil.Process().memory_info().rss / (1024**3)

# ============================================================================
# æµ‹è¯• 1: PyTorch MPS åŸºç¡€åŠŸèƒ½
# ============================================================================
def test_pytorch_mps():
    log("ğŸ”§ æµ‹è¯• 1: PyTorch MPS åŸºç¡€åŠŸèƒ½")
    start_mem = get_mem()
    
    # æ£€æŸ¥ MPS
    assert torch.backends.mps.is_available(), "MPS ä¸å¯ç”¨"
    assert torch.backends.mps.is_built(), "MPS æœªæ„å»º"
    log("  âœ… MPS å¯ç”¨")
    
    # å¼ é‡è¿ç®—
    x = torch.randn(1000, 1000, device='mps')
    y = torch.matmul(x, x)
    log(f"  âœ… å¼ é‡è¿ç®— (1000x1000): {y.shape}")
    
    # æ¢¯åº¦è®¡ç®—
    x = torch.randn(100, 100, device='mps', requires_grad=True)
    y = x.sum()
    y.backward()
    log("  âœ… è‡ªåŠ¨å¾®åˆ†æ­£å¸¸")
    
    # æ··åˆç²¾åº¦
    x = torch.randn(100, 100, device='mps', dtype=torch.float16)
    log(f"  âœ… FP16 æ”¯æŒ: {x.dtype}")
    
    mem = get_mem()
    return {
        "status": "success",
        "mps_available": True,
        "operations": ["matmul", "autograd", "fp16"],
        "memory_gb": mem,
        "elapsed_sec": 0
    }

# ============================================================================
# æµ‹è¯• 2: ç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒ
# ============================================================================
def test_simple_training():
    log("ğŸ”§ æµ‹è¯• 2: ç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒ")
    start_mem = get_mem()
    start_time = time.time()
    
    class TinyNet(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = torch.nn.ModuleList([
                torch.nn.Linear(128, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 128),
                torch.nn.ReLU(),
                torch.nn.Linear(128, 64),
                torch.nn.Linear(64, 10)
            ])
        
        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x
    
    model = TinyNet().to('mps')
    optimizer = torch.optim.Adam(model.parameters())
    criterion = torch.nn.CrossEntropyLoss()
    
    # è®­ç»ƒå¾ªç¯
    losses = []
    for i in range(100):
        x = torch.randn(32, 128, device='mps')
        y = torch.randint(0, 10, (32,), device='mps')
        
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        
        if (i + 1) % 25 == 0:
            log(f"    è¿­ä»£ {i+1}: loss={loss.item():.4f}")
    
    elapsed = time.time() - start_time
    mem = get_mem()
    
    return {
        "status": "success",
        "model": "TinyNet",
        "parameters": sum(p.numel() for p in model.parameters()),
        "iterations": 100,
        "final_loss": losses[-1],
        "avg_loss": sum(losses) / len(losses),
        "memory_gb": mem,
        "elapsed_sec": elapsed
    }

# ============================================================================
# æµ‹è¯• 3: Transformers åŠ è½½
# ============================================================================
def test_transformers():
    log("ğŸ”§ æµ‹è¯• 3: Transformers æ¨¡å‹åŠ è½½")
    start_mem = get_mem()
    start_time = time.time()
    
    from transformers import GPT2LMHeadModel, GPT2Tokenizer
    
    # åŠ è½½ GPT-2 (124M)
    log("  ğŸ“¦ åŠ è½½ GPT-2 (124M)...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token
    
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model = model.to('mps')
    
    # ç”Ÿæˆæµ‹è¯•
    log("  ğŸ¯ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
    input_ids = tokenizer.encode("Hello, I am", return_tensors='pt').to('mps')
    with torch.no_grad():
        output = model.generate(input_ids, max_length=20, do_sample=True)
    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    
    mem = get_mem()
    elapsed = time.time() - start_time
    
    log(f"  ğŸ“ ç”Ÿæˆç¤ºä¾‹: {generated[:50]}...")
    
    return {
        "status": "success",
        "model": "GPT-2",
        "parameters": model.num_parameters(),
        "memory_gb": mem,
        "elapsed_sec": elapsed,
        "sample_output": generated[:100]
    }

# ============================================================================
# æµ‹è¯• 4: PEFT LoRA é…ç½®
# ============================================================================
def test_peft_lora():
    log("ğŸ”§ æµ‹è¯• 4: PEFT LoRA é…ç½®")
    start_mem = get_mem()
    start_time = time.time()
    
    from transformers import AutoModelForCausalLM
    from peft import LoraConfig, get_peft_model, TaskType
    
    # ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•
    model_name = "gpt2"  # ä½¿ç”¨ GPT-2 ä½œä¸ºæµ‹è¯•
    
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model = model.to('mps')
    
    # é…ç½® LoRA
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["c_attn", "c_proj", "c_fc"]
    )
    
    lora_model = get_peft_model(model, lora_config)
    lora_model.print_trainable_parameters()
    
    # ç®€å•è®­ç»ƒæµ‹è¯•
    optimizer = torch.optim.Adam(lora_model.parameters())
    
    for i in range(20):
        x = torch.randint(0, 50257, (4, 32), device='mps')
        y = x.clone()
        
        optimizer.zero_grad()
        outputs = lora_model(input_ids=x, labels=y)
        loss = outputs.loss
        
        if loss.item() > 0 and i < 20:
            loss.backward()
            optimizer.step()
        
        if (i + 1) % 10 == 0:
            log(f"    è¿­ä»£ {i+1}: loss={loss.item():.4f}")
    
    mem = get_mem()
    elapsed = time.time() - start_time
    
    return {
        "status": "success",
        "base_model": "GPT-2",
        "lora_r": 8,
        "trainable_params": lora_model.num_parameters(only_trainable=True),
        "total_params": model.num_parameters(),
        "memory_gb": mem,
        "elapsed_sec": elapsed
    }

# ============================================================================
# æµ‹è¯• 5: MLX åŸç”Ÿæ¡†æ¶
# ============================================================================
def test_mlx():
    log("ğŸ”§ æµ‹è¯• 5: MLX åŸç”Ÿæ¡†æ¶")
    start_mem = get_mem()
    start_time = time.time()
    
    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    
    # æ£€æŸ¥è®¾å¤‡
    devices = mx.list_devices()
    log(f"  ğŸ“± MLX è®¾å¤‡: {devices}")
    
    # ç®€å• MLP
    class MLXNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.layers = [
                nn.Linear(128, 256),
                nn.ReLU(),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, 10)
            ]
        
        def __call__(self, x):
            for layer in self.layers:
                x = layer(x)
            return x
    
    model = MLXNet()
    num_params = sum(v.size for _, v in model.parameters().items())
    log(f"  ğŸ“Š æ¨¡å‹å‚æ•°é‡: {num_params:,}")
    
    # è®­ç»ƒ
    optimizer = optim.Adam(learning_rate=1e-3)
    loss_fn = nn.losses.cross_entropy
    
    losses = []
    for i in range(50):
        x = mx.random.normal((32, 128))
        y = mx.random.randint(0, 10, (32,))
        
        logits = model(x)
        loss = loss_fn(logits, y)
        
        grad = mx.grad(loss, model.parameters())
        optimizer.update(model, grad)
        
        losses.append(float(loss))
        
        if (i + 1) % 25 == 0:
            log(f"    è¿­ä»£ {i+1}: loss={float(loss):.4f}")
    
    elapsed = time.time() - start_time
    mem = get_mem()
    
    return {
        "status": "success",
        "framework": "MLX",
        "parameters": num_params,
        "iterations": 50,
        "final_loss": losses[-1],
        "devices": [str(d) for d in devices],
        "memory_gb": mem,
        "elapsed_sec": elapsed
    }

# ============================================================================
# æµ‹è¯• 6: é‡åŒ–æ¨ç†æµ‹è¯•
# ============================================================================
def test_quantization():
    log("ğŸ”§ æµ‹è¯• 6: é‡åŒ–æ¨ç†æµ‹è¯•")
    start_mem = get_mem()
    start_time = time.time()
    
    from transformers import GPT2LMHeadModel
    import torch.quantization
    
    # åŠ è½½æ¨¡å‹
    model = GPT2LMHeadModel.from_pretrained('gpt2')
    model = model.to('mps')
    
    # åŠ¨æ€é‡åŒ– (int8)
    log("  ğŸ“¦ åº”ç”¨åŠ¨æ€é‡åŒ–...")
    quantized_model = torch.quantization.quantize_dynamic(
        model,
        {torch.nn.Linear},
        dtype=torch.qint8
    )
    
    # æ¨ç†æµ‹è¯•
    log("  ğŸ¯ é‡åŒ–æ¨ç†æµ‹è¯•...")
    input_ids = torch.randint(0, 50257, (1, 32), device='mps')
    
    with torch.no_grad():
        output = quantized_model(input_ids)
    
    mem = get_mem()
    elapsed = time.time() - start_time
    
    return {
        "status": "success",
        "method": "Dynamic Quantization (int8)",
        "model": "GPT-2",
        "memory_gb": mem,
        "elapsed_sec": elapsed
    }

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    print("\n" + "="*70)
    print("    ğŸš€ LLM è®­ç»ƒç¯å¢ƒç²¾ç®€æµ‹è¯•")
    print("    Apple M2 Ultra (64GB) - macOS")
    print("="*70 + "\n")
    
    results = {}
    tests = [
        ("PyTorch_MPS", test_pytorch_mps),
        ("Simple_Training", test_simple_training),
        ("Transformers", test_transformers),
        ("PEFT_LoRA", test_peft_lora),
        ("MLX_Native", test_mlx),
        ("Quantization", test_quantization),
    ]
    
    for name, test_func in tests:
        try:
            log(f"\n{'='*50}")
            result = test_func()
            results[name] = result
            log(f"âœ… {name} å®Œæˆ | å†…å­˜: {result.get('memory_gb', 'N/A'):.2f}GB | æ—¶é—´: {result.get('elapsed_sec', 0):.2f}s")
        except Exception as e:
            log(f"âŒ {name} å¤±è´¥: {e}")
            results[name] = {"status": "error", "error": str(e)}
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "="*70)
    print("    ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*70)
    
    success = [k for k, v in results.items() if v.get("status") == "success"]
    failed = [k for k, v in results.items() if v.get("status") != "success"]
    
    print(f"\nâœ… é€šè¿‡: {len(success)}/{len(tests)}")
    for k in success:
        r = results[k]
        print(f"   â€¢ {k}: {r.get('elapsed_sec', 0):.2f}s, {r.get('memory_gb', 0):.2f}GB")
    
    if failed:
        print(f"\nâŒ å¤±è´¥: {len(failed)}/{len(tests)}")
        for k in failed:
            print(f"   â€¢ {k}: {results[k].get('error', 'Unknown')}")
    
    # ä¿å­˜ç»“æœ
    output_file = os.path.join(REPORT_DIR, "quick_test_results.json")
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nğŸ“ ç»“æœä¿å­˜: {output_file}")
    
    return results

if __name__ == "__main__":
    main()
