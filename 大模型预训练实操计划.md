# 🤖 大模型预训练实操计划

> 基于「预训练面试重点」视频课程
> 目标：一个月从零训练一个 1B-7B 模型，踩完所有核心坑

---

## 📅 整体时间规划

| 阶段 | 周期 | 核心目标 | 产出 |
|------|------|---------|------|
| **预备周** | 第0周 | 环境搭建、工具熟悉 | 能跑通训练框架 |
| **第1周** | 数据工程 | 数据下载、清洗、去重、分词 | 一份干净的训练数据集 |
| **第2周** | 流程+事故 | 跑通流程、故意制造训练事故 | 理解训练稳定性的重要性 |
| **第3周** | 正式训练 | 训练 1B 模型、深度调参 | 一个训练好的 1B 模型 |
| **第4周** | 评估对比 | 评估、对比实验、写复盘 | 面试可讲的实战经验 |

**总预算**：约 3000-5000 元（A100 算力）

---

## 🎯 每周详细计划

### 📆 第0周：环境搭建与工具熟悉（预备周）

**目标**：准备好所有工具，能跑通训练框架

#### 任务清单

##### Day 1-2：算力平台选型与账号注册

| 平台 | 特点 | 价格参考 |
|------|------|---------|
| [AutoDL](https://www.autodl.com) | 国内访问快、便宜 | A100 80G ≈ 2-3 元/时 |
| [阿里云 PAI](https://www.aliyun.com/product/pai) | 国内大厂、稳定性好 | A100 80G ≈ 8-10 元/时 |
| [Lambda Labs](https://lambdalabs.com) | 海外便宜、配置简单 | A100 80G ≈ $1.09/时 |
| [RunPod](https://runpod.io) | 按需付费、灵活 | A100 80G ≈ $1.49/时 |

**实操任务**：
- [ ] 注册 1-2 个算力平台账号
- [ ] 完成实名认证（国内平台需要）
- [ ] 充值 1000-2000 元预算
- [ ] 学会如何创建实例、连接 Jupyter/SSH

##### Day 3-4：训练框架选型与本地跑通

**推荐框架**（按易用性排序）：

| 框架 | 特点 | 推荐度 |
|------|------|-------|
| [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) | NVIDIA 官方、工业级 | ⭐⭐⭐⭐⭐ |
| [DeepSpeed](https://github.com/microsoft/DeepSpeed) | 微软出品、易用性好 | ⭐⭐⭐⭐⭐ |
| [lit-gpt](https://github.com/Lightning-AI/lit-gpt) | 简单极简、适合学习 | ⭐⭐⭐⭐⭐ |
| [transformers](https://github.com/huggingface/transformers) | HuggingFace 官方 | ⭐⭐⭐⭐ |

**实操任务**：
- [ ] 本地（Mac Studio）安装 Python 3.10+ 环境
- [ ] 克隆 lit-gpt 或 DeepSpeed 示例代码
- [ ] 本地跑通一个 100M 小模型的训练流程（用 CPU 或 Metal）
- [ ] 理解训练脚本的基本参数：

```bash
# lit-gpt 示例
python train.py \
  --model_size 1b \
  --train_data ./data/my_dataset \
  --out_dir ./output \
  --devices 1 \
  --precision bf16
```

##### Day 5-7：熟悉监控与调试工具

**必装工具**：

| 工具 | 用途 |
|------|------|
| Weights & Biases (wandb) | 训练可视化、实验追踪 |
| TensorBoard | 查看 loss 曲线、梯度分布 |
| nvitop | GPU 显存、利用率实时监控 |
| htop | 系统资源监控 |

**实操任务**：
- [ ] 注册 wandb 账号，配置 API Key
- [ ] 跑一个 demo 训练，熟悉 wandb 仪表盘
- [ ] 学会用 nvitop 查看 GPU 利用率

---

### 📆 第1周：数据工程（核心中的核心）

> **核心认知**：大厂做预训练的人，80% 的时间花在数据上，不是模型架构。

#### Day 1-2：数据源调研与下载

**开源预训练数据集**：

| 数据集 | 规模 | 特点 |
|--------|------|------|
| [The Stack](https://huggingface.co/datasets/bigcode/the-stack) | 3TB+ | 源代码，适合代码模型 |
| [SkyPile](https://huggingface.co/datasets/Skywork/SkyPile-150A) | 150B tokens | 中文网页数据 |
| [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) | 15T tokens | 高质量网页数据 |
| [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 5T tokens | 过滤后的网页数据 |
| [Wanjuan](https://huggingface.co/datasets/fds-ccpt/WanJuan) | 3B tokens | 中文数据集 |

**实操任务**：
- [ ] 下载 FineWeb 或 SkyPile 的一个小 subset（10GB-50GB）
- [ ] 用 `datasets` 库加载数据，了解数据格式：

```python
from datasets import load_dataset

# 加载数据
ds = load_dataset("HuggingFaceFW/fineweb", name="sample-10BT", split="train")
print(ds[0])
```

#### Day 3-4：数据质量分析

**分析维度**：

| 维度 | 检查内容 | 工具 |
|------|---------|------|
| 重复率 | 重复文本、段落、文档 | MinHash SimHash |
| 长度分布 | 极短/极长文本 | Python 脚本 |
| 语言分布 | 中英文比例 | langdetect |
| 内容质量 | 垃圾信息、广告、敏感内容 | 关键词过滤、正则 |

**实操任务**：
- [ ] 统计数据集的基本信息（样本数、平均长度、token 数）
- [ ] 检测语言分布（用 `langdetect`）
- [ ] 绘制长度分布直方图

```python
# 数据长度分析示例
lengths = [len(sample["text"]) for sample in ds]
print(f"样本数: {len(lengths)}")
print(f"平均长度: {sum(lengths)/len(lengths):.0f}")
print(f"最大长度: {max(lengths)}")
print(f"最小长度: {min(lengths)}")
```

#### Day 5-6：去重（最关键的一步）

**去重方法**：

| 方法 | 原理 | 适用场景 |
|------|------|---------|
| MinHash + LSH | 局部敏感哈希，近似重复检测 | 大规模去重 |
| SimHash | 海明距离检测相似文档 | 精确去重 |
| Exact Match | 完全相同检测 | 简单去重 |

**实操任务**：
- [ ] 用 `datasketch` 实现 MinHash 去重
- [ ] 去重前后对比统计
- [ ] 亲眼看到那些重复内容是如何污染数据的

```python
# MinHash 去重示例
from datasketch import MinHash, MinHashLSH

# 创建 MinHash
minhashes = []
for text in texts:
    m = MinHash(num_perm=128)
    for shingle in text.split():
        m.update(shingle.encode('utf-8'))
    minhashes.append(m)

# LSH 索引
lsh = MinHashLSH(threshold=0.9, num_perm=128)
for i, m in enumerate(minhashes):
    lsh.insert(str(i), m)

# 查询重复
duplicates = []
for i, m in enumerate(minhashes):
    result = lsh.query(m)
    if len(result) > 1:
        duplicates.append((i, result))
```

#### Day 7：分词研究与词表压缩

**分词核心概念**：

| 概念 | 含义 |
|------|------|
| 词表大小 (vocab_size) | 词表中有多少个 token |
| 压缩率 | 文本 token 数 / 字符数，越高越好 |
| 未知词 (unk) | 词表外词汇的处理方式 |

**实操任务**：
- [ ] 比较不同词表（BPE、WordPiece、SentencePiece）
- [ ] 训练一个自己的 SentencePiece 词表
- [ ] 计算压缩率，理解为什么词表设计很重要

```python
# SentencePiece 训练示例
import sentencepiece as spm

# 训练词表
spm.SentencePieceTrainer.train(
    input='corpus.txt',
    model_prefix='my_tokenizer',
    vocab_size=32000,
    character_coverage=1.0,
    model_type='bpe'
)

# 使用词表
sp = spm.SentencePieceProcessor()
sp.load('my_tokenizer.model')
tokens = sp.encode_as_pieces('你好世界')
print(tokens)  # ['▁', '你', '好', '世', '界']
```

**本周交付物**：
- [ ] 一份数据质量报告（重复率、长度分布、语言分布）
- [ ] 去重后的干净数据集
- [ ] 分词压缩率对比分析

---

### 📆 第2周：跑通流程 + 故意制造事故

> **核心认知**：真正的经验来自"制造事故"和"解决问题"，不是"等着 loss 下降"。

#### Day 1-2：小模型跑通完整流程

**推荐配置**（小模型验证流程）：

| 参数 | 小模型配置 | 说明 |
|------|-----------|------|
| 模型大小 | 100M-300M | 能用 CPU 或单卡跑 |
| 数据量 | 1B-10B tokens | 快速迭代 |
| 训练时长 | 1-2 小时 | 验证流程正确 |

**实操任务**：
- [ ] 配置 lit-gpt 或 DeepSpeed 训练脚本
- [ ] 用 100M 模型跑通完整流程（数据 → 训练 → checkpoint → 评估）
- [ ] 记录训练日志，熟悉 wandb 仪表盘

#### Day 3-6：故意制造事故

**必做实验清单**（每个都要亲眼见证）：

| 实验 | 操作 | 预期现象 | 学到了什么 |
|------|------|---------|-----------|
| **学习率灾难** | LR × 10 | Loss 爆炸、NaN | 学习率的重要性 |
| **无 Warmup** | 关掉 warmup | 初期剧烈震荡 | Warmup 的作用 |
| **Batch Size 太小** | batch=1 | 梯度噪声极大、收敛慢 | Batch Size 与梯度噪声 |
| **无 Gradient Clipping** | 关掉 clip | 梯度爆炸、训练崩溃 | 梯度裁剪的作用 |
| **错误 Optimizer** | 用 SGD 代替 AdamW | 收敛极慢 | Optimizer 选择 |
| **Precision 错误** | FP32 换成 FP16 | 显存爆炸或数值错误 | 混合精度配置 |

**实操任务**：
- [ ] 每个实验跑 10-30 分钟，记录 loss 曲线
- [ ] 截图保存异常现象
- [ ] 写出每个事故的原因分析和解决方案

#### Day 7：训练稳定性总结

**问题诊断清单**：

| 现象 | 可能原因 | 解决方案 |
|------|---------|---------|
| Loss 爆炸 | 学习率太大、梯度爆炸 | 调小 LR、开启 clip |
| Loss 不下降 | 学习率太小、卡住了 | 调大 LR、检查数据 |
| Loss 震荡 | Batch size 太小、无 warmup | 增大 batch、开启 warmup |
| 显存不够 | 模型太大、激活值太多 | Gradient checkpointing、梯度累积 |
| MFU 太低 | I/O 瓶颈、数据加载慢 | 优化 DataLoader、用 SSD |

**本周交付物**：
- [ ] 6 个"事故"实验记录（每个包含：操作、现象、原因、解决方案）
- [ ] 训练稳定性最佳实践总结

---

### 📆 第3周：正式训练 1B 模型

> **核心目标**：训练一个真正可用的 1B 模型

#### Day 1-2：配置与启动

**1B 模型训练配置**：

| 参数 | 推荐值 | 说明 |
|------|-------|------|
| 模型大小 | 1B (1024M) | 单卡 A100 可跑 |
| 序列长度 | 2048 或 4096 | 根据显存调整 |
| Batch Size | 8-32 per GPU | 累积到 256-512 global |
| 学习率 | 3e-4 ~ 5e-4 | 经验值，可调 |
| Warmup | 2000-5000 steps | 约 1-2% 总步数 |
| 训练数据量 | 20B tokens | Chinchilla 法则 |
| 预计时长 | 3-7 天 | 24 小时跑 |

**实操任务**：
- [ ] 配置 DeepSpeed 或 Megatron 训练参数
- [ ] 启动训练，监控前 1000 步
- [ ] 确认 loss 曲线正常（初期震荡 → 稳定下降）

```bash
# DeepSpeed 示例配置 (ds_config.json)
{
  "train_batch_size": 256,
  "gradient_accumulation_steps": 1,
  "optimizer": {
    "type": "AdamW",
    "params": {
      "lr": 3e-4,
      "betas": [0.9, 0.95],
      "eps": 1e-8
    }
  },
  "scheduler": {
    "type": "WarmupLR",
    "params": {
      "warmup_min_lr": 0,
      "warmup_max_lr": 3e-4,
      "warmup_num_steps": 2000
    }
  },
  "fp16": {
    "enabled": true
  },
  "zero_optimization": {
    "stage": 1
  }
}
```

#### Day 3-5：日常监控与调优

**每日检查清单**：

| 检查项 | 理想值 | 工具 |
|--------|-------|------|
| GPU 利用率 | > 80% | nvitop |
| 显存占用 | 70-90% | nvitop |
| Loss 曲线 | 平滑下降 | wandb |
| 学习率 | 按调度器变化 | wandb |
| MFU | 40-60% | wandb 计算 |

**常见问题处理**：

| 问题 | 处理方法 |
|------|---------|
| OOM (显存不足) | 减小 batch size、开启 gradient checkpointing |
| Loss spike | 回滚 checkpoint、跳过问题 batch |
| 收敛变慢 | 学习率 decay、检查数据质量 |
| 训练卡住 | 检查 I/O、DataLoader 是否瓶颈 |

#### Day 6-7：Checkpoint 策略与保存

**Checkpoint 保存策略**：

| 策略 | 频率 | 场景 |
|------|------|------|
| 每 N 步保存 | 1000-5000 步 | 快速回滚 |
| 最佳 loss 保存 | loss 最低时 | 选取最佳模型 |
| 定时保存 | 每天凌晨 | 防止数据丢失 |

**实操任务**：
- [ ] 配置自动保存策略
- [ ] 保存 3-5 个 checkpoint
- [ ] 测试从 checkpoint 恢复训练

**本周交付物**：
- [ ] 一个训练到 10-20B tokens 的 1B 模型
- [ ] 完整的训练日志（wandb 截图）
- [ ] 至少 3 个保存完好的 checkpoint

---

### 📆 第4周：评估与对比实验

> **核心目标**：建立「改了什么 → 效果变了多少」的对应关系

#### Day 1-2：评估体系搭建

**必做评估任务**：

| 评估类型 | 任务 | 数据集 |
|---------|------|-------|
| 困惑度 (PPL) | 语言建模能力 | 验证集 |
| 文本生成 | 续写质量 | 自定义 prompts |
| 常识推理 | 问答能力 | HellaSwag、ARC |
| 知识问答 | 事实知识 | MMLU 轻量版 |

**实操任务**：
- [ ] 搭建评估脚本
- [ ] 在验证集上计算 PPL
- [ ] 测试几个基线 prompts

```python
# PPL 计算示例
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("./checkpoint-10000")
tokenizer = AutoTokenizer.from_pretrained("./checkpoint-10000")

inputs = tokenizer(texts, return_tensors="pt", padding=True)
with torch.no_grad():
    outputs = model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss
    ppl = torch.exp(loss)
print(f"PPL: {ppl.item():.2f}")
```

#### Day 3-5：对比实验设计

**必做对比实验**：

| 实验 | 变量 | 预期结论 |
|------|------|---------|
| 数据配比 | 小说 vs 新闻 vs 百科 | 不同数据对能力的影响 |
| 学习率 | 1e-4 vs 3e-4 vs 5e-4 | 最佳学习率是多少 |
| Batch Size | 64 vs 256 vs 512 | 收敛速度 vs 显存 trade-off |
| Warmup | 0 vs 1000 vs 5000 | Warmup 多长合适 |

**实验记录模板**：

```markdown
## 实验名称：XXX 对比

### 实验设置
- 模型：1B
- 数据：XX 数据集
- 其他参数固定：XXX

### 实验结果
| 配置 | 最终 PPL | 收敛步数 | MFU |
|------|---------|---------|-----|
| A | 15.2 | 5000 | 45% |
| B | 14.8 | 4000 | 48% |

### 结论
B 配置最优，原因：...
```

#### Day 6-7：复盘文档与面试素材整理

**复盘文档结构**：

```markdown
# 我的 1B 模型训练复盘

## 1. 基本信息
- 训练时间：2026-XX-XX 到 XX
- 算力成本：约 XXX 元
- 最终 PPL：XX

## 2. 遇到的问题与解决方案
| 问题 | 原因 | 解决 |
|------|------|------|
| Loss 爆炸 | 学习率太大 | 调小 50% |

## 3. 最有价值的发现
- ...

## 4. 如果重做会怎么改进
- ...

## 5. 面试可以讲的点
- ...
```

**本周交付物**：
- [ ] 评估报告（PPL、生成示例、对比实验）
- [ ] 完整复盘文档
- [ ] 1-2 个可以面试时详细讲解的「故事」

---

## 🛠️ 工具与资源清单

### 必备工具

| 类别 | 工具 | 安装 |
|------|------|------|
| 算力平台 | AutoDL / 阿里云 | 网页注册 |
| 训练框架 | lit-gpt / DeepSpeed | `pip install lit-gpt` |
| 数据处理 | datasets, datasketch | `pip install datasets datasketch` |
| 监控 | wandb, nvitop | `pip install wandb nvitop` |
| 分词 | sentencepiece | `pip install sentencepiece` |

### 推荐学习资源

| 资源 | 类型 | 推荐度 |
|------|------|-------|
| [lit-gpt 教程](https://github.com/Lightning-AI/lit-gpt) | 代码教程 | ⭐⭐⭐⭐⭐ |
| [DeepSpeed 文档](https://www.deepspeed.ai/) | 官方文档 | ⭐⭐⭐⭐⭐ |
| [Chinchilla Paper](https://arxiv.org/abs/2203.15556) | 论文 | ⭐⭐⭐⭐ |
| [Megatron-LM 教程](https://github.com/NVIDIA/Megatron-LM) | 代码教程 | ⭐⭐⭐⭐ |
| [WandB 教程](https://docs.wandb.ai/) | 官方文档 | ⭐⭐⭐⭐ |

---

## 📊 成功标准

### 最低成功标准（能达到）
- [ ] 花 < 5000 元完成训练
- [ ] 训练一个能跑通的 1B 模型
- [ ] PPL 收敛到 20 以下
- [ ] 能回答面试常见问题

### 优秀成功标准（值得吹）
- [ ] 花 < 3000 元完成训练
- [ ] 对比实验证明了自己调参的判断
- [ ] 复盘文档被多人参考
- [ ] 面试时能讲出「我亲手踩过的坑」

---

## 📅 启动检查清单

在开始之前，确认以下事项：

- [ ] 算力平台账号注册 + 充值
- [ ] 本地环境配置完成（Python 3.10+）
- [ ] wandb 账号注册 + API Key 配置
- [ ] 数据集下载脚本测试通过
- [ ] 训练框架本地能跑通（100M 模型即可）

---

## 💡 学习建议

### 1. 遇到问题怎么办？

| 优先度 | 资源 |
|-------|------|
| 1 | 查看文档（lit-gpt README、DeepSpeed 文档） |
| 2 | 搜索 GitHub Issues（类似问题） |
| 3 | 查看 wandb 日志（定位问题） |
| 4 | 问 AI（Claude、ChatGPT） |
| 5 | 问社区（GitHub Discussions、HuggingFace Discord） |

### 2. 每天投入时间

| 阶段 | 每日投入 |
|------|---------|
| 第 0-2 周 | 1-2 小时/天（主要是配置和学习） |
| 第 3-4 周 | 30 分钟/天（主要是监控） |
| 周末 | 2-4 小时（做实验、写总结） |

### 3. 心理建设

- **前两周最痛苦**：环境搭建和数据处理很枯燥，但这是最有价值的技能
- **第三周最期待**：看着模型 loss 下降会有成就感
- **第四周最收获**：对比实验教会你「为什么这样调」比「调成什么样」更重要

---

> **最后提醒**：这个计划的目标不是「跑通」，而是「理解」。每一个实验都要问自己「为什么」，每一个问题都要记录解决方案。这些才是面试官真正想听的内容。

---

*计划创建时间：2026-02-09*
*基于「预训练面试重点」视频课程*
